---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dpi = 300,
  fig.align = "center"
)
```

# Day 2: Collecting Digital Trace Data

## Pre-arrival 

- Materiais disponíveis em: https://sicss.io/curriculum

> Importante: eu fiz pequenas modificações nos exercícios para não ficar exatamente igual ao código mostrado. Mas fiz os objetivos propostos :) , ex: coletar dados de uma API e fazer um gráfico, coletar dados do wikipedia, etc.

### What is Digital Trace Data?

- "Pegadas digitais". Novas possibilidades de acompanhar/estudar o comportamento humano em maior escala. Enquanto interagimos com a tecnologia, estamos deixando rastros digitais" do nosso comportamento.

- Mostrou um exemplo de estudo: Eagle et al. 2010. O que podemos aprender com os dados? No estudo mostrado, a diversidade da rede de contatos das pessoas e as pessoas que elas conectavam nas redes sociais foram relacionadas com desigualdade econômica e diversidade.

- Dados de celular! Onde as pessoas estão? Estão em casa? Relacionado com COVID-19: no futuro (talvez próximo) poderemos saber se as pessoas estão respeitando a quarentena (ou outras regras).

- Digital trace data:  são grandes bases de dados digitais, que permitem estudar o comportamento humano em uma escala jamais vista. Exemplo: redes sociais, dados de pesquisa na internet, blogs, dados geo-espaciais, audio-visual, arquivos históricos, registros administrativos. Qualquer tipo de pegada digital que deixamos! Esses dados nos permitem viajar no tempo, é possível acessar uma página da internet no passado.

- Esses dados raramente são "perfeitos". Falamos anteriormente que em CS separamos os dados "custom-made" e os dados "ready-made" (falado anteriormente na [aula 1-1](https://github.com/beatrizmilz/2021-SICSS/blob/master/notes/Day_1-Introduction_and_Ethics/README.md#introduction-to-computational-social-science)). Nesse caso, os dados que usamos aqui são mais "ready-made": são dados criados com um objetivo, e usamos esses dados com outros objetivos.

### Strengths and Weakness of Digital Trace Data

- "Forças" da Digital trace data:

  - Big: tem mais dados, e dados muito grandes! Avanços em poder computacional também permite que a gente consiga coletar/analisar esses dados.
  
  - Always on: sempre disponível. Exemplo: redes sociais apresentam um registro do que está acontecendo on-line. 
  
  - Non-reactive: não resulta de uma interação onde a pessoa pesquisadora pergunta a opinião de uma pessoa que está participando do estudo (ex. entrevista, survey, grupo focal, etc). Isso é bem relevante quando se trata de pesquisas onde as perguntas feitas na entrevista/survey/grupo focal são de temas que possam constranger as pessoas participantes, e que elas não queiram responder.
  
  - Capture social relationships: conseguimos dados das redes sociais. 
  
- "Fraquezas" da Digital trace data:

  - Inaccessible: muito dos dados (e muitos dados valiosos para entender o comportamento humano) atualmente é inacessível, pois estão na posse de grandes empresas (como Google, Apple, Amazon, etc). Muitas vezes elas não podem disponibilizar essas bases de dados por questões legais. 

  - Non-representative: os dados não representam a população inteira. Pensando em redes sociais: % de pessoas da população que usa essa rede, pensar na presença grupos em termos de idade, sexo, urbano/rural, grupos minoritários, etc. Se queremos estudar dados de, por exemplo, twitter, precisamos estar cientes dessas vieses. E essas porcentagens da população nas redes sociais muda ao longo do tempo, e entre as plataformas.  E se os dados conter vieses (bias), e não reflete fielmente a população que estamos tentando estudar? 
  
  - Drifting: as plataformas usadas mudam com o tempo, as pessoas migram de redes sociais (ex: myspace -> facebook). A relação entre as pessoas e as plataformas usadas também muda com o tempo. Então precisamos ter atenção à isso: se queremos estudar um intervalo de tempo, pensar em quais plataformas eram usadas nesse período (e por quem).
  
  - Algorithimically confounded: situação onde um algoritmo dentro de uma plataforma digital é alterado, e isso cria uma mudança no que vemos de comportamento humano. Interpretamos essa mudança como algo significativo, porém pode ser apenas a forma como as pessoas estão interagindo com o algoritmo. A caixa preta de ML: em alguns casos, não sabemos o que está causando algo.
  
  - Unstructured: os dados são bagunçados e não estruturados! A maior parte do tempo das pessoas que trabalham com ciência de dados é usada limpando e arrumando dados!
  
  - Sensitive: muitas pegadas digitais são informações privadas e sensíveis. Ex: dados de redes de relacionamento (ok cupid).
  
  - Incomplete: os dados podem estar incompletos. Em alguns casos está relacionado as regras da plataforma usada para coletar os dados.
  
  - Elite/publication bias: teremos uma fotografia distorcida da realidade. Em alguns casos, as informações encontradas são a visão de pessoas em situação privilegiada (ex comum em documentos históricos). Ex nas redes sociais: as pessoas estão postando coisas para fazer com que elas tenham mais status (look better).
  
  - Positivity-bias: Nas redes sociais, as pessoas postam mais coisas positivas, e não postam tanto as coisas negativas. 
  
  
- O futuro do digital trace data: temos que encontrar formas de nos aproveitar das forças do DTD, ao mesmo tempo reconhecer as fraquezas de DTD (e lidar com isso). As melhores pesquisas futuras em CSC serão híbridas: usando dados ready-made e custom-made.


### Application Programming Interfaces (API)

- O que é uma API? 

  - É uma das principais ferramentas usadas na CSC que quer estudar digital trace data.
  
  - Possibilita coletar dados de algumas redes sociais, mas não apenas!
  
  - A API possibilita alguns grupos de pessoas a acessarem alguns tipos de dados. Os dados disponíveis pode depender devido às credenciais (credentials).
  
  - Muitas APIs não são públicas: existem dentro de uma empresa, ou entre duas companhias para transmitir dados.
  
  - O número de APIs está crescendo! Pesquisar por APIs
    - [Programmable Web](https://www.programmableweb.com/category/all/apis)
    - [Any API](https://any-api.com/)


- Como uma API funciona?

  - Para criar a URL para buscar dados em uma API: Base API URL + Requested Filds/Data (endpoints) + Data format requested (usually JSON) + Query + API Key (credential/token)
  
  - JSON é uma forma eficiente para armazenar dados, e é muito usado em API
  
  - Para usar a API, lemos a documentação para descobrir como criar as URLs que devemos usar. A documentação é um manual. 
  
  - API credential/access token: algumas APIs não pedem credenciais.
  
  - Rate limiting: uma API tem regras sobre a quantidade de dados que é permitido coletar em um intervalo determinado de tempo. Entender isso, e se necessário, colocar pausas no código. Throttling: o site mostra que está perto de atingir o rate limit e retorna os dados cada vez mais devagar.
  
  - Exemplo com twitter: buscar dados na API do twitter. Usar no maximo n = 18 mil. Essa busca com a API simples só busca os dados recentes. O argumento `retryonratelimit` da função `rtweet::search_tweets()` é útil nesse caso. 

- Autenticando:

```{r eval=FALSE, include=TRUE}
# install.packages("rtweet")
library(magrittr, include.only = "%>%")

# A forma de realizar a autenticação mostrada no video 
# estava gerando um erro. Nas issues do pacote, encontrei
# outras pessoas com o mesmo erro. Uma das sugestões que
# encontrei por lá é essa função bearer_token(), 
# que funcionou bem :)

rtweet::bearer_token()
```

- Buscando os tweets

```{r  eval=FALSE, include=TRUE}
# Fiz a pesquisa com outro termo..
tweets <- rtweet::search_tweets(
  q = "rstats", # query para buscar
  n = 4000,  # número de tweets para coletar
  include_rts = FALSE # queremos os RTs ou não?
  )

# salvei o resultado pra nao precisar ficar coletando
# sempre que eu executar esse código (é demorado)

readr::write_rds(tweets, file = "exemplo_tweets.Rds")
```



```{r}
# abrindo os tweets salvos
tweets_salvos <- readr::read_rds("exemplo_tweets.Rds")

# carregando apenas o pipe:
library(magrittr, include.only = "%>%") 

# não quero ver a base toda, tem muitas colunas!
tweets_salvos %>% 
  dplyr::select(created_at, screen_name, text) 
```

- Quais são as colunas presentes?
```{r}
names(tweets_salvos)
```
- Criar um gráfico

```{r grafico_tweets, dpi = 300}
dia_min <- format(min(tweets_salvos$created_at), "%d/%m")
dia_max <- format(max(tweets_salvos$created_at), "%d/%m")

tweets_salvos %>% 
  rtweet::ts_plot(by = "hours") +
  ggplot2::theme_minimal() + 
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(x = "Data", y = "Número de tweets",
                title = glue::glue("Frequência de tweets sobre rstats, entre os dias {dia_min} e {dia_max}"),
                subtitle = "Contagem de tweets agregado por hora",
                caption = "\nFonte: Dados coletados da API do Twitter, através do pacote {rtweet}.")
```

- Ver os tweets em uma região (EUA)

```{r eval=FALSE, include=TRUE}
geo_tweets <- rtweet::search_tweets(
  q = "rstats", # query para buscar
  n = 4000,  # número de tweets para coletar
  include_rts = FALSE, # queremos os RTs ou não?
  geocode = rtweet::lookup_coords("usa"),
  type = "recent"
  )

# salvei o resultado pra nao precisar ficar coletando
# sempre que eu executar esse código (é demorado)

readr::write_rds(geo_tweets, file = "exemplo_geo_tweets.Rds")
```

```{r}
# abrindo os tweets salvos
tweets_geo_salvos <- readr::read_rds("exemplo_geo_tweets.Rds")

# arrumando as colunas lat long
geocoded <- rtweet::lat_lng(tweets_geo_salvos)
```


```{r mapa_tweets_maps}
# plotando o mapa com o pacote maps
par(mar = c(0, 0, 0, 0))
maps::map("world", lwd = .25)
with(geocoded, points(
  lng,
  lat,
  pch = 20,
  cex = .50,
  col = rgb(0, .3, .7, .75)
))
```

```{r mapa_tweets_sf}
world <-
  rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")

sf_usa <-
  world %>% dplyr::filter(admin == "United States of America")

geocoded_filtrado_sf <- geocoded %>%
  tidyr::drop_na(lat, lng) %>%
  sf::st_as_sf(coords = c("lng", "lat"), crs = 4326)

dia_min <- format(min(geocoded_filtrado_sf$created_at), "%d/%m")
dia_max <- format(max(geocoded_filtrado_sf$created_at), "%d/%m")


  geocoded_filtrado_sf %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(data = sf_usa) +
  ggplot2::geom_sf(color = rgb(0, .3, .7, .75)) +
  ggplot2::theme_bw() +
  ggplot2::coord_sf(xlim = c(-127, -65),
                    ylim = c(23, 51),
                    expand = FALSE) +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = "Longitude",
    y = "Latitude",
    title = glue::glue(
      "Tweets sobre rstats entre os dias {dia_min} e {dia_max}"
    ),
    subtitle = "Tweets que informaram a localização, e postados nos EUA",
    caption = "\nFonte: Dados coletados da API do Twitter, através do pacote {rtweet}."
  )
```

- No exemplo, mostrou mais coisas que podemos fazer com o pacote rtweet. 

- Função pra checar o rate limit: 

```{r}
head(rtweet::rate_limit()[50:55, 1:4])
```

-  Postar tweets: `rtweet::post_tweet()`

- Podemos fazer um loop para repetir a busca :)

- APIs podem ser usadas não apenas para coletar dados, mas também para visualização/análise/modelagem


- Desafios ao trabalhar com APIs:

  - Muitos dados ainda não estão disponíveis publicamente.
  
  - Cada API tem seus padrões, então aprender a usar uma API significa que precisamos ler com cuidado a documentação.
  

- O final do video está repetido. 


### Screen Scraping

- Screen-scraping: tentar extrair dados de uma página da internet, e adicionar em uma base de dados.

- Imagem interessante [no slide](https://sicss.io/2020/materials/day2-digital-trace-data/screenscraping/Rpres/Screenscraping.html#/2)

- Sreen-scraping é permitido por lei? O melhor é buscar os dados em uma API. Consultar os termos de serviço do site que queremos fazer a raspagem, e também o arquivo robots.txt  . Além disso, ele aconselha conversar com a equipe da universidade que pode dar conselhos relacionados à questão legal.

- Screen-scraping é frustrante

- Ler a página web no R: pacote útil para isso é o {rvest}

Exercício: iremos buscar dados do wikipedia.

Busquei outra página, para ficar diferente do exemplo dado :)

Caso alguém queira baixar os dados a seguir, ver: [wiki_naruto_shippuden.Rds](wiki_naruto_shippuden.Rds)

```{r eval=FALSE, include=TRUE}
url_wiki <-
  "https://pt.wikipedia.org/wiki/Lista_de_epis%C3%B3dios_de_Naruto_Shippuden"

wikipedia_page  <- rvest::read_html(url_wiki)

buscar_tabela <- function(n_tabela, n_season) {
  rvest::html_node(
    wikipedia_page,
    xpath = glue::glue('//*[@id="mw-content-text"]/div[1]/table[{n_tabela}]')
  ) %>%
    rvest::html_table() %>%
    janitor::clean_names() %>%
    dplyr::transmute(
      temporada = n_season,
      n_ep = as.character(no),
      titulo = titulo_original,
      data_de_estreia
    )
  }

tabela_temporadas <- seq(4, 42, by = 2)

tabela_suja <-
  purrr::map2_dfr(tabela_temporadas, 1:20, buscar_tabela)


eps_naruto_shippuden <- tabela_suja %>%
  tidyr::separate(n_ep, into = c("n_ep", "tipo_episodio"), "\\(") %>%
  dplyr::mutate(
    n_ep = readr::parse_number(n_ep),
    data_de_estreia = readr::parse_date(
      data_de_estreia,
      format = "%d de %B de %Y",
      locale = readr::locale("pt")
    ),
    tipo_episodio = stringr::str_replace_all(tipo_episodio, "\\)", "")
  ) %>%
  tidyr::drop_na(n_ep) %>%
  tibble::rowid_to_column() %>%
  dplyr::filter(rowid %% 2 != 0) %>%
  dplyr::select(-rowid)  %>% 
  dplyr::mutate(tipo_episodio = dplyr::case_when(
    is.na(tipo_episodio) ~ "Canon",
    tipo_episodio == "½filler" ~ "Semi-filler",
    TRUE ~ tipo_episodio
  ))

readr::write_rds(eps_naruto_shippuden, "wiki_naruto_shippuden.Rds")
```

```{r}
eps_naruto_shippuden <- readr::read_rds("wiki_naruto_shippuden.Rds")

eps_naruto_shippuden %>%
  knitr::kable()

eps_naruto_shippuden %>%
  dplyr::count(tipo_episodio, name = "quantidade_episodios", sort = TRUE)
```

- RSelenium: precisa do java e docker. 

- Screen scraping pode ser muito útil em alguns casos: quando não tem API e também não tem problemas legais.

- Ficar de olho nos padrões das URLs.

- O número de sites onde é permitido legalmente fazer screen scraping está diminuindo.


### Building Apps and Bots for Social Science Research

- Relembrando as fraquezas de digital trace data: incomplete, inaccessible, non-representative, drifting, algorithmic confounding, dirty, sensitive.

- Quais são as alternativas?

  - Survey: taxa de resposta costuma ser baixa atualmente;
  
  - A maioria das nossas perguntas requer dados longitudinais, quantitativos, relacionais.
  
  - Digital trace data: apresenta algumas vantagens que as fontes mais comuns não apresentam, como: big, always on, non-reactive. Apesar de ter desvantagens, as vantagens são muito boas!
  
  - Mostrou um exemplo de estudo hídbrido: teve o uso de social trace data, e também de surveys para complementar os dados. 
  
  - Social Media Survey Apps: uma ferramenta para web ou celular, construída pelas pessoas pesquisadoras para: 
  
    - Coletar dados públicos e/ou privados produzidos por pessoas usuárias de redes sociais, através de uma API.
    
    - Coletar informações suplementares destas pessoas usuárias (Ex. dados demográficos) utilizando métodos mais convencionais de realizar surveys;
    
    - Oferecer algo em retorno para as pessoas usuárias, para incentivar a ideia de compartilhar os dados (ex. alguma análise ou incentivos financeiros)
    
- Desafios ao construir apps para pesquisas na ciências sociais:

  - É necessário alguns conhecimentos técnicos, como: html, css, computação em nuvem, reactive programming
  
  - É um ambiente onde se compete pela atenção (os apps não são mais "novos")
  
  - Preocupações sobre o compartilhamento e privacidade dos dados
  
  - Incentivos convincentes são difíceis de identificar - e particularmente desafiador em estudos de temas sensíveis. Mas os incentivos financeiros podem ser uma opção importante para avançar.
  
- Shiny apps!  
    
- Bots tem uma má reputação, por terem sido usados para espalhar fake news! Porém ao ser desenvolvido de forma ética, pode ser útil para pesquisas em CSC. Existem diversas questões éticas sobre isso. 
    
