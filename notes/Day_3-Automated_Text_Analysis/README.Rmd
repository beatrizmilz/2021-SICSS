---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dpi = 300,
  fig.align = "center",
  cache = TRUE
)
```

# Day 3: Automated Text Analysis

## Pre-arrival 

- Materiais disponíveis em: https://sicss.io/curriculum

### An Introduction to Text Analysis

- Análise de texto quantitativa; de forma automatizada e usando uma grande quantidade de dados.

- História: registros mais antigos indicam que foi usado para estudar propaganda na primeira guerra mundial. Foi similar ao que conhecemos como uma análise de conteúdo.

- Leituras recomendadas:
  - [ ] [Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20)

  - [ ] [Machine Translation: Mining Text for Social Theory](https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-081715-074206)

### Text Analysis Basics

- Encoding dos caracteres: o uso de diferentes encodings muda com o tempo, entre os sistemas operacionais, etc. O uso de UTF-8 tem crescido!

- GREP: Globally search a regular expression and print

- Expressões regulares (Regular expressions, ou RegEx): padrões que descrevem um conjunto de strings


```{r}
duke_web_scrape <- "Duke Experts: A Trusted Source for Policymakers\n\n\t\t\t\t\t\t"

# verificar se uma palavra está no texto
# R base
grepl("Experts", duke_web_scrape)
# tidyverse
stringr::str_detect(duke_web_scrape, "Experts")

# Substituir \t por nada
# R base
gsub("\t", "", duke_web_scrape)
# tidyverse
stringr::str_replace_all(duke_web_scrape, "\t", "")

# Substituir \t e \n por nada
# R base
gsub("\t|\n", "", duke_web_scrape)
# tidyverse
stringr::str_replace_all(duke_web_scrape, "\t|\n", "")
```
- More GREP!

```{r}
library(magrittr, include.only = "%>%")
some_text <- c("Friends", "don't", "let", "friends", "make", "wordclouds")

# buscar todas as palavras que iniciam com
# F maiúsculo
# R Base
some_text[grep("^[F]", some_text)]
# tidyverse (é melhor se for trabalhar com dfs)
some_text %>%
  tibble::as_tibble() %>%
  dplyr::filter(stringr::str_detect(value, "^[F]"))
```

- [Cheat sheet RegEx](https://github.com/rstudio/cheatsheets/raw/master/regex.pdf)
- [Cheat sheet stringr](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

- Escaping text - com a barra invertida!

```{r}
text_chunk <- c("[This Professor is not so Great]")

# não funciona
# gsub("[", "", text_chunk)

# funciona - base R
gsub("\\[|\\]", "", text_chunk)

# versão tidyverse
stringr::str_replace_all(text_chunk,"\\[|\\]", "")
```

- **Unidades de análise**: o que vai contar como uma palavra?

- Tokenization: o mais comum é tokenizar por palavras. Também é possível tokenizar por N-grams: sequências de palavras de comprimento N. Exemplos:

  - N = 1 - Unigrams - this, is, a, sentence

  - N = 2 - Bigrams - this is, is a, a sentence

  - N = 3 - Trigrams - this is a, is a sentence

- Criando bases de dados textuais: costumamos trabalhar com um grande grupo de documentos (como livros, artigos de jornais, tweets, etc)

```{r}
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))

trumptweets$text[1]
```

- Criar um "corpus style document": preservar o conteúdo e os metadados.

```{r}
# install.packages("tm")

trump_corpus <- tm::Corpus(tm::VectorSource(as.vector(trumptweets$text)))

trump_corpus
```

- Outra abordagem: tidytext!

```{r}

tidy_trump_tweets <- trumptweets %>% 
  dplyr::select(created_at, text) %>% 
  tidytext::unnest_tokens("word", text)

tidy_trump_tweets %>% 
  head() %>% 
  knitr::kable()
```

```{r}
tidy_trump_tweets %>% 
  dplyr::count(word, sort = TRUE)
```


- **Pré-processamento de texto**

- Stopwords: palavras que não adicionam significado para o texto. É comum remover as stopwords das análises.

```{r}
tidy_trump_tweets_without_stop_words <- 
  tidy_trump_tweets %>% 
  dplyr::anti_join(tidytext::stop_words)

tidy_trump_tweets_without_stop_words  %>% 
  dplyr::count(word, sort = TRUE)

```

- Limpar o texto: ele mostrou com R Base, estou fazendo com tidyverse
```{r}
tidy_trump_tweets_filtered <-
  tidy_trump_tweets_without_stop_words %>%
  dplyr::mutate(
    # remover números
    word = tm::removeNumbers(word),
    
    # substituir vírgula e ponto por nada
    word = stringr::str_replace_all(word, ",|\\.", ""),
    
    # transformar em minusculas
    word = stringr::str_to_lower(word),
    
    # Remover espaços desnecessários
    word = stringr::str_trim(word),
    
    # transformar strings vazias em NA
    word = dplyr::if_else(word == "", NA_character_, word)
    
  ) %>%
  tidyr::drop_na(word)
```

- Stemização: reduzir cada palavra para sua forma mais básica, para fazer a análise

```{r}
tidy_trump_tweets_stem <- tidy_trump_tweets_filtered %>%
  dplyr::mutate(word = SnowballC::wordStem(words = word, language = "en"))

tidy_trump_tweets_stem  %>% 
  dplyr::count(word, sort = TRUE)
```

- Matriz: document-term matrix


```{r}
tidy_trump_tweets_stem %>% 
  dplyr::count(created_at, word) %>% 
  tidytext::cast_dtm(created_at, word, n)
```

> Livro [Text Mining with R - A Tidy approach](https://www.tidytextmining.com/)

> Livro [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)

### Dictionary-Based Text Analysis


### Topic Models


### Text Networks