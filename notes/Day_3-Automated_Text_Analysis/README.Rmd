---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dpi = 300,
  fig.align = "center"
)
```

# Day 3: Automated Text Analysis

## Pre-arrival 

- Materiais disponíveis em: https://sicss.io/curriculum

### An Introduction to Text Analysis

- Análise de texto quantitativa; de forma automatizada e usando uma grande quantidade de dados.

- História: registros mais antigos indicam que foi usado para estudar propaganda na primeira guerra mundial. Foi similar ao que conhecemos como uma análise de conteúdo.

- Leituras recomendadas:
  - [ ] [Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20)

  - [ ] [Machine Translation: Mining Text for Social Theory](https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-081715-074206)

### Text Analysis Basics

- Encoding dos caracteres: o uso de diferentes encodings muda com o tempo, entre os sistemas operacionais, etc. O uso de UTF-8 tem crescido!

- GREP: Globally search a regular expression and print

- Expressões regulares (Regular expressions, ou RegEx): padrões que descrevem um conjunto de strings


```{r}
duke_web_scrape <- "Duke Experts: A Trusted Source for Policymakers\n\n\t\t\t\t\t\t"

# verificar se uma palavra está no texto
# R base
grepl("Experts", duke_web_scrape)
# tidyverse
stringr::str_detect(duke_web_scrape, "Experts")

# Substituir \t por nada
# R base
gsub("\t", "", duke_web_scrape)
# tidyverse
stringr::str_replace_all(duke_web_scrape, "\t", "")

# Substituir \t e \n por nada
# R base
gsub("\t|\n", "", duke_web_scrape)
# tidyverse
stringr::str_replace_all(duke_web_scrape, "\t|\n", "")
```
- More GREP!

```{r}
library(magrittr, include.only = "%>%")
some_text <- c("Friends", "don't", "let", "friends", "make", "wordclouds")

# buscar todas as palavras que iniciam com
# F maiúsculo
# R Base
some_text[grep("^[F]", some_text)]
# tidyverse (é melhor se for trabalhar com dfs)
some_text %>%
  tibble::as_tibble() %>%
  dplyr::filter(stringr::str_detect(value, "^[F]"))
```

- [Cheat sheet RegEx](https://github.com/rstudio/cheatsheets/raw/master/regex.pdf)
- [Cheat sheet stringr](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

- Escaping text - com a barra invertida!

```{r}
text_chunk <- c("[This Professor is not so Great]")

# não funciona
# gsub("[", "", text_chunk)

# funciona - base R
gsub("\\[|\\]", "", text_chunk)

# versão tidyverse
stringr::str_replace_all(text_chunk,"\\[|\\]", "")
```

- **Unidades de análise**: o que vai contar como uma palavra?

- Tokenization: o mais comum é tokenizar por palavras. Também é possível tokenizar por N-grams: sequências de palavras de comprimento N. Exemplos:

  - N = 1 - Unigrams - this, is, a, sentence

  - N = 2 - Bigrams - this is, is a, a sentence

  - N = 3 - Trigrams - this is a, is a sentence

- Criando bases de dados textuais: costumamos trabalhar com um grande grupo de documentos (como livros, artigos de jornais, tweets, etc)

```{r}
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))

trumptweets$text[1]
```

- Criar um "corpus style document": preservar o conteúdo e os metadados.

```{r}
# install.packages("tm")

trump_corpus <- tm::Corpus(tm::VectorSource(as.vector(trumptweets$text)))

trump_corpus
```

- Outra abordagem: tidytext!

```{r}

tidy_trump_tweets <- trumptweets %>% 
  dplyr::select(created_at, text) %>% 
  tidytext::unnest_tokens("word", text)

tidy_trump_tweets %>% 
  head() %>% 
  knitr::kable()
```

```{r}
tidy_trump_tweets %>% 
  dplyr::count(word, sort = TRUE)
```


- **Pré-processamento de texto**

- Stopwords: palavras que não adicionam significado para o texto. É comum remover as stopwords das análises.

```{r}
tidy_trump_tweets_without_stop_words <- 
  tidy_trump_tweets %>% 
  dplyr::anti_join(tidytext::stop_words) %>% 
  dplyr::filter(!word %in% c("https", "rt", "t.co", "amp"))

tidy_trump_tweets_without_stop_words  %>% 
  dplyr::count(word, sort = TRUE)

```

- Limpar o texto: ele mostrou com R Base, estou fazendo com tidyverse
```{r}
tidy_trump_tweets_filtered <-
  tidy_trump_tweets_without_stop_words %>% 
  dplyr::mutate(
    # remover números
    word = tm::removeNumbers(word), 
    # word =  gsub("\\d+", "", word), #opcao sem tm
    
    # substituir vírgula e ponto por nada
    word = stringr::str_replace_all(word, ",|\\.", ""),
    
    # transformar em minusculas
    word = stringr::str_to_lower(word),
    
    # Remover espaços desnecessários
    word = stringr::str_trim(word),
    
    # transformar strings vazias em NA
    word = dplyr::if_else(word == "", NA_character_, word)
    
  ) %>%
  tidyr::drop_na(word)
```

- Stemização: reduzir cada palavra para sua forma mais básica, para fazer a análise

```{r}
tidy_trump_tweets_stem <- tidy_trump_tweets_filtered %>%
  dplyr::mutate(word = SnowballC::wordStem(words = word, language = "en"))

tidy_trump_tweets_stem  %>% 
  dplyr::count(word, sort = TRUE)
```

- Matriz: document-term matrix


```{r }
tidy_trump_tweets_stem %>% 
  dplyr::count(created_at, word) %>% 
  tidytext::cast_dtm(created_at, word, n)
```

> Livro [Text Mining with R - A Tidy approach](https://www.tidytextmining.com/)

> Livro [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)

### Dictionary-Based Text Analysis

- Formas sofisticadas de contagem de palavras, associadas com algum lexicon (ou grupo de palavras) que contém algum tipo de significado.

```{r}
top_words <- tidy_trump_tweets_filtered %>% 
    dplyr::count(word, sort = TRUE)


# EDIT: MOMENTO EM QUE INSTALEI O R 4.1.0!
# a partir deste ponto pode ser que o pipe nativo seja usado.



top_words |>
  dplyr::slice(1:20) |>
  dplyr::mutate(word = forcats::fct_reorder(word, n)) |> 
  ggplot2::ggplot() +
  ggplot2::geom_col(ggplot2::aes(
    x = word,
    y = n,
    fill = word
  )) +
  ggplot2::scale_fill_viridis_d(direction = -1) +
  ggplot2::coord_flip() +
  ggplot2::theme_minimal() +
  ggplot2::guides(fill = FALSE) +
  ggplot2::labs(y = "Frequência", x = "Palavras", title = "Palavras mais frequentes nos tweets do Trump") +
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5, size = 18))
```


- Term frequency inverse document frequency (tf-idf) (isso não tá claro pra mim ainda)
  - Inverse document frequency (IDF): Give more weight to a term occurring in less documents. 
  - Função `bind_ft_idf()`


```{r}
trump_tf_idf <- tidy_trump_tweets_filtered |> 
  dplyr::count(word, created_at) |>
  tidytext::bind_tf_idf(word, created_at, n) |>
  dplyr::arrange(desc(tf_idf))

trump_tf_idf |> head(10) |> knitr::kable()
```


Criando um dicionário relacionado a um tema. Ex:

```{r}
economic_dictionary <- c("economy", "unemployment", "trade", "tariffs")

economic_tweets <- trumptweets |> 
  dplyr::filter(stringr::str_detect(string = text, pattern =  economic_dictionary))

head(economic_tweets$text, 2)
```
- Análise de sentimento: porcentagem de palavras negativas e positivas.


```{r}
head(tidytext::get_sentiments(lexicon = "bing"))
```
```{r}
trump_tweet_sentiment <- tidy_trump_tweets_filtered |> 
  dplyr::inner_join(tidytext::get_sentiments(lexicon = "bing")) |> 
  dplyr::count(created_at, sentiment)

head(trump_tweet_sentiment)
```


```{r}
trump_tweet_sentiment |> 
  dplyr::mutate(date = lubridate::floor_date(created_at, "day"))   |> 
  dplyr::filter(sentiment == "negative") |> 
  dplyr::count(date, sentiment) |> 
  ggplot2::ggplot() +
  ggplot2::geom_line(ggplot2::aes(x = date, y = n), color = "red", size = 0.5) +
  ggplot2::theme_minimal() + 
  ggplot2::labs(x = "Data", y = "Número de palavras negativas", title = "Sentimentos negativos nos tweets do Trump") +
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5, size = 18))
```

  - Quem decide o que é uma palavra negativa ou positiva? Quem criou o dicionário!

  - Coisas como sarcasmo, ironia: são difíceis de detectar nesse tipo de análise. 
  
  - Quando escolher um dicionário, escolher algo que tenha entendimentos similares ao meu. Diferentes dicionários tem melhores performances em diferentes contextos.
  
  
- B: Em português, conheço esse pacote de análise de sentimentos:

```{r}
# devtools::install_github("sillasgonzaga/lexiconPT")
library(lexiconPT)
# carregando os dicionários
data("sentiLex_lem_PT02")
data("oplexicon_v2.1")
data("oplexicon_v3.0")

lexiconPT::get_word_sentiment("temer")

lexiconPT::get_word_sentiment("bolsonaro")
```

  
- Linguistic inquiry word count (LIWC) - é popular, mas imperfeito.

- Quando usar dictionary-based analises? depende! Se sabemos concretamente o que vamos procurar nos dados, e temos uma lista de palavras, pode ser uma boa idea. Se não, podemos usar os métodos não supervisionados. Também é possível fazer com métodos híbridos :)



### Topic Models

- Técnicas para identificar temas latentes em um corpus (grupo de arquivos de texto que estamos usando).

  - Leituras amplificadas!

  - A técnica não diz para nós quantos tópicos estão presentes no corpus, nós precisamos informar isso para a técnica. 

  - O resultado da técnica é: 1) uma lista de cada palavra associada a um tópico, e 2) relação de cada documento para cada tópico.

  - Document-term matrix: representação de um corpus em formato de matriz, as linhas representam documentos, e as colunas são qualquer palavra  que aparece no corpus. As células da matrix contam quantas vezes cada palavra aparece em cada documento.


```{r}
library(topicmodels)
library(tm)
data("AssociatedPress")
tm::inspect(AssociatedPress[1:5,1:5])

dim(AssociatedPress)[1] # número de linhas (neste caso, representam documentos)
dim(AssociatedPress)[2] # número de colunas (neste caso, representam palavras)
```
  - A primeira vez que realizamos o topic model, costuma estar errado. Vamos testar com 10 tópicos! Depois podemos repetir e testar. Nada substitui a validação feita por seres humanos.

```{r, cache=TRUE}
ap_topic_model <- topicmodels::LDA(AssociatedPress, # mariz que usaremos
                                   k = 10, # numero de topicos  
                                   control = list(seed = 321) # isso é importante para que a análise seja reprodutível 
                                   # (obtenha o mesmo resultado cada vez que executar a função)
                                    )
```


```{r}
ap_topis <- tidytext::tidy(ap_topic_model, matrix = "beta")

ap_top_terms <- ap_topis |> 
  dplyr::group_by(topic) |> 
  dplyr::top_n(10, beta) |> 
  dplyr::ungroup() |> 
  dplyr::arrange(topic, -beta)

ap_top_terms |> 
  dplyr::mutate(term = forcats::fct_reorder(term, beta),
                topic = paste("Topic #", topic)) |> 
  ggplot2::ggplot() +
  ggplot2::geom_col(ggplot2::aes(x = term, y = beta, fill = factor(topic)), show.legend = FALSE) + 
  ggplot2::facet_wrap(~ topic, scales = "free") + 
  ggplot2::theme_minimal() + 
  ggplot2::coord_flip() +
  ggplot2::labs(title = "Topic Model of AP News Articles",
                caption = "Top terms by topic (betas)",
                y = "Betas", x = "Term")+
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5, size = 18),
                 axis.text.x =  ggplot2::element_text(size = 6),
                 axis.text.y =  ggplot2::element_text(size = 6))
  

```

  - O que pode ser um tópico?? Depende em como definimos as coisas. 

  - Resistir à denteção de deixar a técnica fazer tudo. 


- Structural topic modeling: similar ao LDA, mas explora metadados de um arquivo para melhorar a classificação de tópicos em um corpus. Ajuda a entender melhor o resultado.

```{r}
google_doc_id <- "1LcX-JnpGB0lU1iDnXnxB6WFqBywUKpew" # google file ID
poliblogs <-
  read.csv(
    sprintf(
      "https://docs.google.com/uc?id=%s&export=download",
      google_doc_id 
    ),
    stringsAsFactors = FALSE
  )

head(poliblogs)
```

```{r}
# estou com problema com esse pacote no R 4.1.0
# library(stm)
# processed <- stm::textProcessor(poliblogs$documents, metadata = poliblogs)
# out <-  stm::prepDocuments(processed$documents, processed$vocab, processed$meta)
# docs <- out$documents
# vocab <- out$vocab
# meta <-out$meta

# First_STM <- stm::stm(documents = out$documents, vocab = out$vocab,
#               K = 10, prevalence =~ rating + s(day) ,
#               max.em.its = 75, data = out$meta,
#               init.type = "Spectral", verbose = FALSE)

# plot(First_STM)

# stm::findThoughts(First_STM, texts = poliblogs$documents,
#      n = 2, topics = 3)

# findingk <- stm::searchK(out$documents, out$vocab, K = c(10:30),
#  prevalence =~ rating + s(day), data = meta, verbose=FALSE)

# plot(findingk)
```

- Trabalhando com metadados: não é porque temos algum metadado, que significa que ele deva ser incluido na stm.

```{r}
# predict_topics<-estimateEffect(formula = 1:10 ~ rating + s(day), stmobj = First_STM, metadata = out$meta, uncertainty = "Global")
# 
# plot(predict_topics, covariate = "rating", topics = c(3, 5, 9),
#  model = First_STM, method = "difference",
#  cov.value1 = "Liberal", cov.value2 = "Conservative",
#  xlab = "More Conservative ... More Liberal",
#  main = "Effect of Liberal vs. Conservative",
#  xlim = c(-.1, .1), labeltype = "custom",
#  custom.labels = c('Topic 3', 'Topic 5','Topic 9'))
```

- LDAviz - possibilida criar visualizações interessantes


- Topic models para textos pequenos (como tweets). O LDA não funciona bem com tweets. Tem outra técnica que tem sido usada: stLDA-C, presume que cada tweet só pode conter um tópico.

- Estar ciente das limitações, em alguns casos a análise dictionary based seria o suficiente.

### Text Networks



## Informações da sessão

```{r}
sessioninfo::session_info()
```

