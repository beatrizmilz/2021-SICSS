---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dpi = 300,
  fig.align = "center",
  cache = TRUE
)
```

# Day 3: Automated Text Analysis

## Pre-arrival 

- Materiais disponíveis em: https://sicss.io/curriculum

### An Introduction to Text Analysis

- Análise de texto quantitativa; de forma automatizada e usando uma grande quantidade de dados.

- História: registros mais antigos indicam que foi usado para estudar propaganda na primeira guerra mundial. Foi similar ao que conhecemos como uma análise de conteúdo.

- Leituras recomendadas:
  - [ ] [Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20)

  - [ ] [Machine Translation: Mining Text for Social Theory](https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-081715-074206)

### Text Analysis Basics

- Encoding dos caracteres: o uso de diferentes encodings muda com o tempo, entre os sistemas operacionais, etc. O uso de UTF-8 tem crescido!

- GREP: Globally search a regular expression and print

- Expressões regulares (Regular expressions, ou RegEx): padrões que descrevem um conjunto de strings


```{r}
duke_web_scrape <- "Duke Experts: A Trusted Source for Policymakers\n\n\t\t\t\t\t\t"

# verificar se uma palavra está no texto
# R base
grepl("Experts", duke_web_scrape)
# tidyverse
stringr::str_detect(duke_web_scrape, "Experts")

# Substituir \t por nada
# R base
gsub("\t", "", duke_web_scrape)
# tidyverse
stringr::str_replace_all(duke_web_scrape, "\t", "")

# Substituir \t e \n por nada
# R base
gsub("\t|\n", "", duke_web_scrape)
# tidyverse
stringr::str_replace_all(duke_web_scrape, "\t|\n", "")
```
- More GREP!

```{r}
library(magrittr, include.only = "%>%")
some_text <- c("Friends", "don't", "let", "friends", "make", "wordclouds")

# buscar todas as palavras que iniciam com
# F maiúsculo
# R Base
some_text[grep("^[F]", some_text)]
# tidyverse (é melhor se for trabalhar com dfs)
some_text %>%
  tibble::as_tibble() %>%
  dplyr::filter(stringr::str_detect(value, "^[F]"))
```

- [Cheat sheet RegEx](https://github.com/rstudio/cheatsheets/raw/master/regex.pdf)
- [Cheat sheet stringr](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

- Escaping text - com a barra invertida!

```{r}
text_chunk <- c("[This Professor is not so Great]")

# não funciona
# gsub("[", "", text_chunk)

# funciona - base R
gsub("\\[|\\]", "", text_chunk)

# versão tidyverse
stringr::str_replace_all(text_chunk,"\\[|\\]", "")
```

- **Unidades de análise**: o que vai contar como uma palavra?

- Tokenization: o mais comum é tokenizar por palavras. Também é possível tokenizar por N-grams: sequências de palavras de comprimento N. Exemplos:

  - N = 1 - Unigrams - this, is, a, sentence

  - N = 2 - Bigrams - this is, is a, a sentence

  - N = 3 - Trigrams - this is a, is a sentence

- Criando bases de dados textuais: costumamos trabalhar com um grande grupo de documentos (como livros, artigos de jornais, tweets, etc)

```{r}
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))

trumptweets$text[1]
```

- Criar um "corpus style document": preservar o conteúdo e os metadados.

```{r eval=FALSE, include=TRUE}
# install.packages("tm")

trump_corpus <- tm::Corpus(tm::VectorSource(as.vector(trumptweets$text)))

trump_corpus
```

- Outra abordagem: tidytext!

```{r}

tidy_trump_tweets <- trumptweets %>% 
  dplyr::select(created_at, text) %>% 
  tidytext::unnest_tokens("word", text)

tidy_trump_tweets %>% 
  head() %>% 
  knitr::kable()
```

```{r}
tidy_trump_tweets %>% 
  dplyr::count(word, sort = TRUE)
```


- **Pré-processamento de texto**

- Stopwords: palavras que não adicionam significado para o texto. É comum remover as stopwords das análises.

```{r}
tidy_trump_tweets_without_stop_words <- 
  tidy_trump_tweets %>% 
  dplyr::anti_join(tidytext::stop_words) %>% 
  dplyr::filter(!word %in% c("https", "rt", "t.co", "amp"))

tidy_trump_tweets_without_stop_words  %>% 
  dplyr::count(word, sort = TRUE)

```

- Limpar o texto: ele mostrou com R Base, estou fazendo com tidyverse
```{r}
tidy_trump_tweets_filtered <-
  tidy_trump_tweets_without_stop_words %>% 
  dplyr::mutate(
    # remover números
    # word = tm::removeNumbers(word), # nao esta funcionando no 4.1.0
    word =  gsub("\\d+", "", word),
    
    # substituir vírgula e ponto por nada
    word = stringr::str_replace_all(word, ",|\\.", ""),
    
    # transformar em minusculas
    word = stringr::str_to_lower(word),
    
    # Remover espaços desnecessários
    word = stringr::str_trim(word),
    
    # transformar strings vazias em NA
    word = dplyr::if_else(word == "", NA_character_, word)
    
  ) %>%
  tidyr::drop_na(word)
```

- Stemização: reduzir cada palavra para sua forma mais básica, para fazer a análise

```{r}
tidy_trump_tweets_stem <- tidy_trump_tweets_filtered %>%
  dplyr::mutate(word = SnowballC::wordStem(words = word, language = "en"))

tidy_trump_tweets_stem  %>% 
  dplyr::count(word, sort = TRUE)
```

- Matriz: document-term matrix


```{r eval=FALSE, include=TRUE}
tidy_trump_tweets_stem %>% 
  dplyr::count(created_at, word) %>% 
  tidytext::cast_dtm(created_at, word, n)
```

> Livro [Text Mining with R - A Tidy approach](https://www.tidytextmining.com/)

> Livro [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)

### Dictionary-Based Text Analysis

- Formas sofisticadas de contagem de palavras, associadas com algum lexicon (ou grupo de palavras) que contém algum tipo de significado.

```{r}
top_words <- tidy_trump_tweets_filtered %>% 
    dplyr::count(word, sort = TRUE)


top_words |>
  dplyr::slice(1:20) |>
  dplyr::mutate(word = forcats::fct_reorder(word, n)) |> 
  ggplot2::ggplot() +
  ggplot2::geom_col(ggplot2::aes(
    x = word,
    y = n,
    fill = word
  )) +
  ggplot2::scale_fill_viridis_d(direction = -1) +
  ggplot2::coord_flip() +
  ggplot2::theme_minimal() +
  ggplot2::guides(fill = FALSE) +
  ggplot2::labs(y = "Frequência", x = "Palavras", title = "Palavras mais frequentes nos tweets do Trump") +
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5, size = 18))
```


- Term frequency inverse document frequency (tf-idf) (isso não tá claro pra mim ainda)
  - Inverse document frequency (IDF): Give more weight to a term occurring in less documents. 
  - Função `bind_ft_idf()`


```{r}
trump_tf_idf <- tidy_trump_tweets_filtered |> 
  dplyr::count(word, created_at) |>
  tidytext::bind_tf_idf(word, created_at, n) |>
  dplyr::arrange(desc(tf_idf))

trump_tf_idf |> head(10) |> knitr::kable()
```


Criando um dicionário relacionado a um tema. Ex:

```{r}
economic_dictionary <- c("economy", "unemployment", "trade", "tariffs")

economic_tweets <- trumptweets |> 
  dplyr::filter(stringr::str_detect(string = text, pattern =  economic_dictionary))

head(economic_tweets$text, 2)
```
- Análise de sentimento: porcentagem de palavras negativas e positivas.


```{r}
head(tidytext::get_sentiments(lexicon = "bing"))
```
```{r}
trump_tweet_sentiment <- tidy_trump_tweets_filtered |> 
  dplyr::inner_join(tidytext::get_sentiments(lexicon = "bing")) |> 
  dplyr::count(created_at, sentiment)

head(trump_tweet_sentiment)
```


```{r}
trump_tweet_sentiment |> 
  dplyr::mutate(date = lubridate::floor_date(created_at, "day"))   |> 
  dplyr::filter(sentiment == "negative") |> 
  dplyr::count(date, sentiment) |> 
  ggplot2::ggplot() +
  ggplot2::geom_line(ggplot2::aes(x = date, y = n), color = "red", size = 0.5) +
  ggplot2::theme_minimal() + 
  ggplot2::labs(x = "Data", y = "Número de palavras negativas", title = "Sentimentos negativos nos tweets do Trump") +
  ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5, size = 18))
```

  - Quem decide o que é uma palavra negativa ou positiva? Quem criou o dicionário!

  - Coisas como sarcasmo, ironia: são difíceis de detectar nesse tipo de análise. 
  
  - Quando escolher um dicionário, escolher algo que tenha entendimentos similares ao meu. Diferentes dicionários tem melhores performances em diferentes contextos.
  
  
- B: Em português, conheço esse pacote de análise de sentimentos:

```{r}
# devtools::install_github("sillasgonzaga/lexiconPT")
library(lexiconPT)
# carregando os dicionários
data("sentiLex_lem_PT02")
data("oplexicon_v2.1")
data("oplexicon_v3.0")

lexiconPT::get_word_sentiment("temer")

lexiconPT::get_word_sentiment("bolsonaro")
```

  
- Linguistic inquiry word count (LIWC) - é popular, mas imperfeito.

- Quando usar dictionary-based analises? depende! Se sabemos concretamente o que vamos procurar nos dados, e temos uma lista de palavras, pode ser uma boa idea. Se não, podemos usar os métodos não supervisionados. Também é possível fazer com métodos híbridos :)



### Topic Models


### Text Networks




```{r}
sessioninfo::session_info()$platform
```

